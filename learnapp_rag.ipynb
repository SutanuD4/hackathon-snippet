{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa/I9jpo/FA2uAwXDksv7x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diN8X6Of5a0o"
      },
      "outputs": [],
      "source": [
        "!pip -q install fastapi uvicorn nest_asyncio pyngrok --upgrade\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install pymupdf networkx openai sentence-transformers spacy\n",
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#redacted\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
      ],
      "metadata": {
        "id": "PvPg74Ri5oUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, sys, json, pickle, math, time, threading\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path.cwd()\n",
        "STORE = BASE / \"store\"\n",
        "STORE.mkdir(exist_ok=True)\n",
        "\n",
        "PDF_NAME = \"gecu101.pdf\"\n",
        "PDF_PATH = BASE / PDF_NAME\n",
        "\n",
        "print(\"Working dir:\", BASE)\n",
        "print(\"Artifacts dir:\", STORE)\n",
        "print(\"Expected PDF path:\", PDF_PATH)"
      ],
      "metadata": {
        "id": "U0THkhRV6Bsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Global config\n",
        "CHUNK_TOKENS = 500  # ~ words granularity (simple split)\n",
        "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Embedding model (torch-backed)\n",
        "embedder = SentenceTransformer(EMB_MODEL)\n",
        "\n",
        "def pdf_to_text(path: Path) -> str:\n",
        "    doc = fitz.open(str(path))\n",
        "    text = \" \".join(page.get_text() for page in doc)\n",
        "    return text\n",
        "\n",
        "def text_to_chunks(text: str, tokens_per_chunk: int = CHUNK_TOKENS) -> List[str]:\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+tokens_per_chunk]) for i in range(0, len(words), tokens_per_chunk)]\n",
        "\n",
        "def embed_texts(texts: List[str]) -> torch.Tensor:\n",
        "    # Returns a (N, D) tensor\n",
        "    return embedder.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "# --------- GPT-nano client (triples extraction) ----------\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_openai_client():\n",
        "    key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not key:\n",
        "        print(\"⚠️ OPENAI_API_KEY not set. Triple extraction and answering will use a local mock.\")\n",
        "        return None\n",
        "    return OpenAI(api_key=key)\n",
        "\n",
        "def extract_triples_llm(text: str, client: Any) -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Calls GPT-nano to extract triples. Falls back to a dumb heuristic if no API key.\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        # Simple heuristic fallback (find \"X ... Y ... Z\" patterns) — very rough!\n",
        "        triples = []\n",
        "        lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "        for l in lines[:10]:\n",
        "            if \" is \" in l and \" by \" in l:\n",
        "                # e.g., \"ice is melted by heat\" -> (\"heat\",\"melts\",\"ice\")\n",
        "                try:\n",
        "                    left, right = l.split(\" is \", 1)\n",
        "                    pred, tail = right.split(\" by \", 1)\n",
        "                    triples.append((tail.strip().strip(\".\"), pred.strip(), left.strip().strip(\".\")))\n",
        "                except:\n",
        "                    pass\n",
        "        return triples\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an information extraction system. From the TEXT, extract knowledge triples as JSON:\n",
        "[{{\"h\":\"head\",\"r\":\"relation\",\"t\":\"tail\"}}, ...]\n",
        "Only include pedagogically relevant scientific facts and processes.\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    out = resp.choices[0].message.content\n",
        "    # Try to parse a JSON array in the output; fallback to line-based parsing.\n",
        "    import json, re\n",
        "    triples: List[Tuple[str,str,str]] = []\n",
        "    try:\n",
        "        json_txt = re.search(r\"\\[.*\\]\", out, flags=re.S).group(0)  # type: ignore\n",
        "        arr = json.loads(json_txt)\n",
        "        for item in arr:\n",
        "            h, r, t = item.get(\"h\",\"\").strip(), item.get(\"r\",\"\").strip(), item.get(\"t\",\"\").strip()\n",
        "            if h and r and t:\n",
        "                triples.append((h,r,t))\n",
        "    except Exception:\n",
        "        # Line-based parsing: look for (h, r, t)\n",
        "        for line in out.splitlines():\n",
        "            if \"(\" in line and \",\" in line and \")\" in line:\n",
        "                clean = line.strip().strip(\"()\")\n",
        "                parts = [p.strip() for p in clean.split(\",\")]\n",
        "                if len(parts) == 3:\n",
        "                    triples.append(tuple(parts))  # type: ignore\n",
        "    return triples\n",
        "\n",
        "def build_kg_from_chunks(chunks: List[str]) -> Tuple[nx.MultiDiGraph, Dict[str, set]]:\n",
        "    \"\"\"\n",
        "    Builds a KG as MultiDiGraph with edges labeled by 'relation' and 'chunk_id'.\n",
        "    Returns: (graph, node_to_chunks mapping)\n",
        "    \"\"\"\n",
        "    G = nx.MultiDiGraph()\n",
        "    node_to_chunks: Dict[str, set] = {}\n",
        "    client = get_openai_client()\n",
        "\n",
        "    for i, ch in enumerate(chunks):\n",
        "        triples = extract_triples_llm(ch, client)\n",
        "        for (h, r, t) in triples:\n",
        "            h, r, t = h.strip(), r.strip(), t.strip()\n",
        "            if not (h and r and t):\n",
        "                continue\n",
        "            G.add_node(h, type=\"entity\")\n",
        "            G.add_node(t, type=\"entity\")\n",
        "            G.add_edge(h, t, relation=r, chunk_id=i)\n",
        "            node_to_chunks.setdefault(h, set()).add(i)\n",
        "            node_to_chunks.setdefault(t, set()).add(i)\n",
        "\n",
        "    return G, node_to_chunks"
      ],
      "metadata": {
        "id": "XpUNEJNw5tW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pickle, torch, networkx as nx\n",
        "from pathlib import Path\n",
        "\n",
        "REG = STORE / \"subjects.json\"\n",
        "\n",
        "def _save_subject_registry(registry: dict):\n",
        "    REG.write_text(json.dumps(registry, indent=2))\n",
        "\n",
        "def _load_subject_registry() -> dict:\n",
        "    if REG.exists():\n",
        "        return json.loads(REG.read_text())\n",
        "    return {}\n",
        "\n",
        "def ingest_subject(subject_name: str, pdf_paths: list, tokens_per_chunk: int = CHUNK_TOKENS):\n",
        "    \"\"\"\n",
        "    Build per-subject artifacts by concatenating all provided PDFs for that subject.\n",
        "    \"\"\"\n",
        "    subject_dir = STORE / subject_name\n",
        "    subject_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Concatenate all PDFs\n",
        "    full_texts = []\n",
        "    for p in pdf_paths:\n",
        "        p = Path(p)\n",
        "        assert p.exists(), f\"Missing PDF: {p}\"\n",
        "        full_texts.append(pdf_to_text(p))\n",
        "    full_text = \"\\n\\n\".join(full_texts)\n",
        "\n",
        "    # Chunk + embed + KG\n",
        "    chunks = text_to_chunks(full_text, tokens_per_chunk=tokens_per_chunk)\n",
        "    doc_embs = embed_texts(chunks)\n",
        "    kg, node_to_chunks = build_kg_from_chunks(chunks)\n",
        "\n",
        "    # Save\n",
        "    torch.save(doc_embs, subject_dir / \"docs.pt\")\n",
        "    pickle.dump(chunks, open(subject_dir / \"chunks.pkl\",\"wb\"))\n",
        "    pickle.dump(kg, open(subject_dir / \"kg.pkl\",\"wb\"))\n",
        "    pickle.dump(node_to_chunks, open(subject_dir / \"node_to_chunks.pkl\",\"wb\"))\n",
        "\n",
        "    # Update registry\n",
        "    reg = _load_subject_registry()\n",
        "    reg[subject_name] = {\n",
        "        \"pdfs\": [str(Path(p)) for p in pdf_paths],\n",
        "        \"chunks\": len(chunks)\n",
        "    }\n",
        "    _save_subject_registry(reg)\n",
        "\n",
        "    print(f\"✅ Ingested subject='{subject_name}' with {len(chunks)} chunks across {len(pdf_paths)} PDF(s).\")"
      ],
      "metadata": {
        "id": "khxU1-Lz5z8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subjects = {\n",
        "    \"science\": [\"gecu101.pdf\"],                     # your uploaded NCERT sample (already there)\n",
        "    \"math\":    [\"gegp101.pdf\"],\n",
        "    \"history\": [\"gees101.pdf\"]\n",
        "}\n",
        "\n",
        "for subj, pdfs in subjects.items():\n",
        "    ingest_subject(subj, pdfs)"
      ],
      "metadata": {
        "id": "iKpnsvi76E3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Subject-aware retrieval (load on demand + routing)\n",
        "import json, pickle, torch, torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "SUBJECT_CACHE = {}  # subject -> dict(chunks, embs, kg, node_to_chunks)\n",
        "SUBJECT_CENTROIDS = {}  # subject -> mean embedding\n",
        "\n",
        "def _load_subject(subj: str):\n",
        "    if subj in SUBJECT_CACHE:\n",
        "        return SUBJECT_CACHE[subj]\n",
        "\n",
        "    subject_dir = STORE / subj\n",
        "    assert subject_dir.exists(), f\"Unknown subject '{subj}'. Did you ingest it?\"\n",
        "\n",
        "    chunks = pickle.load(open(subject_dir / \"chunks.pkl\",\"rb\"))\n",
        "    embs   = torch.load(subject_dir / \"docs.pt\")\n",
        "    kg     = pickle.load(open(subject_dir / \"kg.pkl\",\"rb\"))\n",
        "    node_to_chunks = pickle.load(open(subject_dir / \"node_to_chunks.pkl\",\"rb\"))\n",
        "\n",
        "    SUBJECT_CACHE[subj] = {\n",
        "        \"chunks\": chunks,\n",
        "        \"embs\": embs,\n",
        "        \"kg\": kg,\n",
        "        \"node_to_chunks\": node_to_chunks,\n",
        "    }\n",
        "    # centroid for routing\n",
        "    with torch.no_grad():\n",
        "        SUBJECT_CENTROIDS[subj] = embs.mean(dim=0)\n",
        "    return SUBJECT_CACHE[subj]\n",
        "\n",
        "def list_subjects():\n",
        "    reg = _load_subject_registry()\n",
        "    return list(reg.keys())\n",
        "\n",
        "def vector_search_subject(subj: str, query: str, top_k: int = 5):\n",
        "    ctx = _load_subject(subj)\n",
        "    chunks, embs = ctx[\"chunks\"], ctx[\"embs\"]\n",
        "    q_emb = embedder.encode([query], convert_to_tensor=True)\n",
        "    sims = F.cosine_similarity(q_emb, embs)\n",
        "    topk = torch.topk(sims, k=min(top_k, sims.numel()))\n",
        "    results = [(int(i), chunks[int(i)], float(sims[int(i)])) for i in topk.indices]\n",
        "    return results\n",
        "\n",
        "def kg_expand_subject(subj: str, query: str, depth: int = 1, n_passages: int = 6):\n",
        "    ctx = _load_subject(subj)\n",
        "    chunks, embs, kg, node_to_chunks = ctx[\"chunks\"], ctx[\"embs\"], ctx[\"kg\"], ctx[\"node_to_chunks\"]\n",
        "\n",
        "    q_emb = embedder.encode([query], convert_to_tensor=True)\n",
        "    sims = F.cosine_similarity(q_emb, embs)\n",
        "    idx = int(torch.argmax(sims))\n",
        "\n",
        "    seed_entities = [node for node, cids in node_to_chunks.items() if idx in cids]\n",
        "\n",
        "    expanded_nodes = set()\n",
        "    for node in seed_entities:\n",
        "        if node in kg:\n",
        "            expanded_nodes |= set(nx.single_source_shortest_path_length(kg, node, cutoff=depth).keys())\n",
        "\n",
        "    candidate_chunks = set()\n",
        "    for node in expanded_nodes:\n",
        "        if node in node_to_chunks:\n",
        "            candidate_chunks |= node_to_chunks[node]\n",
        "\n",
        "    selected_idx = list(candidate_chunks)[:n_passages]\n",
        "    return [(i, chunks[i]) for i in selected_idx]\n",
        "\n",
        "def dual_retrieve_subject(subj: str, query: str, top_k: int = 5, depth: int = 1, n_passages: int = 6):\n",
        "    v = vector_search_subject(subj, query, top_k=top_k)\n",
        "    k = kg_expand_subject(subj, query, depth=depth, n_passages=n_passages)\n",
        "    return v, k\n",
        "\n",
        "def route_subject(query: str) -> str:\n",
        "    \"\"\"Pick subject by nearest centroid; fallback to first if tie/empty.\"\"\"\n",
        "    if not SUBJECT_CENTROIDS:\n",
        "        # lazy load all subjects once\n",
        "        for s in list_subjects():\n",
        "            _load_subject(s)\n",
        "\n",
        "    if not SUBJECT_CENTROIDS:\n",
        "        return None\n",
        "\n",
        "    q_emb = embedder.encode([query], convert_to_tensor=True)\n",
        "    best_subj, best_sim = None, -1e9\n",
        "    for subj, centroid in SUBJECT_CENTROIDS.items():\n",
        "        sim = F.cosine_similarity(q_emb, centroid.unsqueeze(0)).item()\n",
        "        if sim > best_sim:\n",
        "            best_sim, best_subj = sim, subj\n",
        "    return best_subj\n",
        "\n",
        "def dual_retrieve_across_all(query: str, top_k: int = 5, depth: int = 1, n_passages: int = 6):\n",
        "    \"\"\"Search each subject; return the result from the subject whose top-1 similarity is highest.\"\"\"\n",
        "    candidates = []\n",
        "    for subj in list_subjects():\n",
        "        ctx = _load_subject(subj)\n",
        "        embs = ctx[\"embs\"]\n",
        "        q_emb = embedder.encode([query], convert_to_tensor=True)\n",
        "        sims = F.cosine_similarity(q_emb, embs)\n",
        "        top_idx = int(torch.argmax(sims))\n",
        "        top_sim = float(sims[top_idx])\n",
        "        candidates.append((top_sim, subj, top_idx))\n",
        "\n",
        "    if not candidates:\n",
        "        return None, [], []\n",
        "\n",
        "    candidates.sort(reverse=True)\n",
        "    _, best_subj, _ = candidates[0]\n",
        "    v, k = dual_retrieve_subject(best_subj, query, top_k=top_k, depth=depth, n_passages=n_passages)\n",
        "    return best_subj, v, k"
      ],
      "metadata": {
        "id": "3Jz1fhaQ6hx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def call_gpt_nano(prompt: str) -> str:\n",
        "    key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not key:\n",
        "        # Fallback: simple echo with truncation (for offline demo)\n",
        "        return \"MOCK_ANSWER:\\n\" + prompt[:800]\n",
        "    client = OpenAI(api_key=key)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.4,\n",
        "        max_tokens=600\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def build_prompt(question: str, vec_ctx, kg_ctx):\n",
        "    vec_text = \"\\n\\n\".join([f\"[vec:{i}] {t}\" for i, t, _ in vec_ctx])\n",
        "    kg_text  = \"\\n\\n\".join([f\"[kg:{i}] {t}\" for i, t in kg_ctx])\n",
        "    prompt = f\"\"\"You are an expert tutor. Cite the provided contexts when relevant.\n",
        "Question: {question}\n",
        "\n",
        "Similarity-based context:\n",
        "{vec_text}\n",
        "\n",
        "KG-expanded context:\n",
        "{kg_text}\n",
        "\n",
        "Answer step-by-step, and end with a brief recap + 2 practice questions.\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "i6yz3Es860C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FastAPI app (multi-subject)\n",
        "import nest_asyncio, uvicorn\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import threading\n",
        "\n",
        "app = FastAPI(title=\"KG-RAG (Torch) API — Multi-subject\")\n",
        "\n",
        "class AskReq(BaseModel):\n",
        "    q: str\n",
        "    top_k: int = 5\n",
        "    depth: int = 1\n",
        "    n_passages: int = 6\n",
        "    subject: str | None = None       # optional: pin to a subject\n",
        "    cross_subject: bool = False      # if True, route & search across all\n",
        "\n",
        "class IngestReq(BaseModel):\n",
        "    subject: str\n",
        "    pdf_paths: list[str]\n",
        "\n",
        "@app.get(\"/subjects\")\n",
        "def subjects():\n",
        "    return {\"subjects\": list_subjects()}\n",
        "\n",
        "@app.post(\"/ingest_subject\")\n",
        "def ingest_api(req: IngestReq):\n",
        "    ingest_subject(req.subject, req.pdf_paths)\n",
        "    return {\"status\": \"ok\", \"subject\": req.subject}\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "def ask(req: AskReq):\n",
        "    # 1) choose subject\n",
        "    subj = req.subject\n",
        "    if req.cross_subject or not subj:\n",
        "        # auto-route across all if requested or no subject given\n",
        "        routed = route_subject(req.q)\n",
        "        if routed is None:\n",
        "            return {\"error\": \"No subjects available. Please ingest first.\"}\n",
        "        subj = routed\n",
        "\n",
        "    # 2) retrieve within chosen subject\n",
        "    vec_ctx, kg_ctx = dual_retrieve_subject(subj, req.q, top_k=req.top_k, depth=req.depth, n_passages=req.n_passages)\n",
        "\n",
        "    # 3) build prompt & answer\n",
        "    prompt = build_prompt(req.q, vec_ctx, kg_ctx)\n",
        "    answer = call_gpt_nano(prompt)\n",
        "\n",
        "    return {\n",
        "        \"subject\": subj,\n",
        "        \"question\": req.q,\n",
        "        \"vector_ctx\": [{\"chunk_id\": i, \"score\": score, \"text\": t} for i, t, score in vec_ctx],\n",
        "        \"kg_ctx\": [{\"chunk_id\": i, \"text\": t} for i, t in kg_ctx],\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "def run_server():\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
        "\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "print(\"FastAPI started on http://127.0.0.1:8000\")"
      ],
      "metadata": {
        "id": "MkXrQwMM6mgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List subjects\n",
        "import requests\n",
        "requests.get(\"http://127.0.0.1:8000/subjects\").json()"
      ],
      "metadata": {
        "id": "H6DwJjvv62DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask within a pinned subject\n",
        "requests.post(\"http://127.0.0.1:8000/ask\",\n",
        "              json={\"q\":\"Explain eclipses\", \"subject\":\"science\"}).json()"
      ],
      "metadata": {
        "id": "ta5C1JLi-PlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask across all subjects (auto-routing)\n",
        "requests.post(\"http://127.0.0.1:8000/ask\",\n",
        "              json={\"q\":\"Derive area of a triangle\", \"cross_subject\": True}).json()"
      ],
      "metadata": {
        "id": "TvGV5apr-MFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}